---
title: 'ISA 591 Final Project Part 1: EDA'
author: "Harrison Cradduck, Nick Berry"
date: "`r Sys.Date()`"
output:
  html_document:
    code_folding: show
    df_print: paged
    number_sections: yes
    theme: readable
    toc: yes
    toc_float: yes
    code_download: yes
  word_document:
    toc: no
---

```{r setup, include = FALSE, message = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
knitr::opts_chunk$set(fig.width = 8, fig.height = 6)

if(require(pacman) == 0)
   {install.packages("pacman")}
pacman::p_load(devtools, caret, cluster, dplyr, fastDummies, leaps, pacman, tidyverse, skimr, fastDummies, GGally, DataExplorer, ggrepel, ggthemes, dslabs, lubridate, stringr, tidytext, purrr)

if (!require(mlba)) {
  library(devtools)
  install_github("gedeck/mlba/mlba", force = TRUE)
}
pacman::p_load(mlba, tidyverse)
```

# Introduction

In this project, our ultimate goal is to predict whether a loan applicant will default on their loan based on historical loan and applicant data. Loan default is a significant risk for financial institutions, and accurately predicting the likelihood of default can help lenders make more informed decisions about granting loans. By applying data mining techniques, we will develop predictive models that help to assess an applicant’s risk of default based on various factors.

# Data Dictionary

| Variable               | Description                                                                 |
|------------------------|-----------------------------------------------------------------------------|
| loan_default           | Whether the loan has defaulted. Values: "Yes", "No".                        |
| loan_amnt              | The listed amount of the loan applied for by the borrower.                  |
| term                   | The number of payments on the loan (36 or 60 months).                       |
| int_rate               | Interest rate on the loan.                                                  |
| installment            | The monthly payment owed by the borrower if the loan originates.            |
| grade                  | Loan grade assigned by Lending Club.                                        |
| sub_grade              | Loan subgrade assigned by Lending Club.                                     |
| emp_title              | Job title supplied by the borrower.                                         |
| emp_length             | Employment length in years ("< 1 year" to "10+ years").                     |
| home_ownership         | Home ownership status: RENT, OWN, MORTGAGE, OTHER.                          |
| annual_inc             | Self-reported annual income of the borrower.                                |
| verification_status    | Whether income was verified: Verified, Not Verified, etc.                   |
| issue_d                | The month when the loan was funded.                                         |
| purpose                | Category provided by the borrower for the loan request.                     |
| title                  | Loan title provided by the borrower.                                        |
| dti                    | Debt-to-income ratio.                                                       |
| earliest_cr_line       | Month of the borrower’s earliest reported credit line.                      |
| open_acc               | Number of open credit lines.                                                |
| pub_rec                | Number of derogatory public records.                                        |
| revol_bal              | Total revolving balance.                                                    |
| revol_util             | Revolving line utilization rate (credit used / total credit).               |
| total_acc              | Total number of credit lines in the borrower’s credit file.                 |
| initial_list_status    | Initial listing status of the loan: f (whole) or w (fractional).            |
| application_type       | Type of application: individual, joint, direct pay.                         |
| mort_acc               | Number of mortgage accounts.                                                |
| pub_rec_bankruptcies   | Number of public record bankruptcies.                                       |
| address                | Borrower’s address.                                                         |
| delinq_2yrs            | Number of 30+ days delinquency incidents in past 2 years.                   |
| fico_range_low         | Lower end of FICO score range.                                              |
| fico_range_high        | Upper end of FICO score range.                                              |
| inq_last_6mths         | Number of credit inquiries in the past 6 months.                            |
| mths_since_last_delinq | Months since last delinquency (NA if never).                                |
| last_credit_pull_d     | Most recent date credit was pulled.                                         |
| acc_now_delinq         | Number of accounts currently delinquent.                                    |
| hardship_flag          | Whether borrower is under hardship plan (Y/N).                              |
| debt_settlement_flag   | Whether borrower is in a debt settlement program (Y/N).                     |

# Data Loading

```{r}
# read in .csv file and convert all strings to factors
df <- read.csv("train.csv", stringsAsFactors = TRUE)
head(df)
```

# Exploratory Data Analysis

The goal of this phase is to explore the dataset, understand its structure, detect any patterns or anomalies, and clean the data to prepare it for model building. Proper EDA and data preprocessing are essential for enhancing the quality of the dataset and ensuring the models can learn effectively from the data.

## Data Overview

### Summary Statistics

```{r}
# skim the data
skimr::skim(df)
```

### Missing Data

```{r}
# view missing values
DataExplorer::plot_missing(df)
```

## Data Visualization

### Frequency Plots

```{r}
# view frequencies of categorical variables with fewer levels
DataExplorer::plot_bar(df, by="loan_default", nrow=2, ncol=3)
```

### Histogram Plots

```{r}
# view distributions of numeric variables
DataExplorer::plot_histogram(df, nrow=3, ncol=3)
```

### Correlation Matrix

```{r}
# correlation plot of continuous variables
DataExplorer::plot_correlation(df,
                               type = "continuous",
                               cor_args = list("use" = "pairwise.complete.obs"))
```

### Violin Plots

```{r}
# select numeric columns
num_cols <- df %>%
  dplyr::select(where(is.numeric)) %>%
  names()

# loop through numeric columns and plot
plots <- map(num_cols, function(col) {
  ggplot(df, aes(x = loan_default, y = .data[[col]], fill = loan_default)) +
    geom_violin(trim = FALSE) + # set plots as violin-shaped
    scale_y_log10() + # accounts for skewed values
    labs(title = paste("Distribution of", col, "by Loan Default"),
         y = col, x = "Loan Default") +
    theme_minimal()
})

# display plots
# plots[1:18]
```

**Above is code to create and display violin plots for each numeric variable in the data split by `loan_default`. For the sake of space, the plots are not shown. A look at the plots would show that `int_rate`, `open_acc`, `revol_util`, `fico_range_low`, and `fico_range_high` demonstrate the largest visual differences in distributions for values of "Yes" and "No" of `loan_default`, indicating that these variables may hold some predictive power in predicting `loan_default`in future modeling.**

## Data Cleaning

This section includes removing duplicate data, manually setting base levels for factor variables, filling in blank values, combining levels of factor variables, and removing variables from the data all together.

```{r}
# remove duplicates from data
df <- df %>% distinct()
```

**After exploring the data, we found multiple rows that are duplicated in the data. Likely, these are partners, spouses, or family members included on the same loan application or on a separate, identical loan application. We chose to keep the unique rows in the data and remove all duplicates.**

```{r}
# set "INDIVIDUAL" as the base level for `application_type`
df$application_type <- fct_relevel(df$application_type, "INDIVIDUAL")

levels(df$application_type)
```

**We chose to relevel `application_type` so that the highest frequency level, "INDIVIDUAL", is set as the base level.**

```{r}
# collapse `purpose` into the top 10 levels in terms of frequency ("other" includes all less frequent levels)
df$purpose <- forcats::fct_collapse(df$purpose, OTHER = c("renewable_energy", "educational", "wedding", "house", "other"))

# set the level with the highest frequency as the base level for `purpose`
df$purpose <- forcats::fct_relevel(df$purpose, names(which.max(table(df$purpose)))) # set base to most frequent level

table(df$purpose)
```

**We chose to relevel `purpose` so that the highest frequency level ("debt_consolidation") is set as the base level.**

```{r}
# classify blanks as "unreported" in `emp_length`
df$emp_length <- forcats::fct_recode(df$emp_length, Unreported = "")

table(df$emp_length)
```

**In addition to the year values of `emp_length`, there are 11037 blank values. We chose to replace these blank values with "unreported" as an indicator of emptiness.**

```{r}
# set "10+ years" as the base level for `emp_length`
df$emp_length <- fct_relevel(df$emp_length, "10+ years")

levels(df$emp_length)
```

**We chose to relevel `emp_length` so that the highest frequency level, "10+ years", is set as the base level.**

```{r}
# classify "NONE" as "OTHER" in `home_ownership`
df$home_ownership <- forcats::fct_collapse(df$home_ownership, OTHER = c("OTHER", "NONE"))

table(df$home_ownership)
```

**The "NONE" and "OTHER" categories of `home_ownership` are extremely sparse compared to `MORTGAGE`, `RENT`, and `OWN. We chose to collapse "OTHER" and "NONE" into the "OTHER" category to reduce the number of levels and mitigate data sparsity.**

```{r}
# remove `hardship_flag`
df <- df %>% dplyr::select(-hardship_flag)
```

**We chose to remove `hardship_flag` from the data since it is a constant column across all observations, so it contains no predictive power.**

```{r}
# remove `installment`
df <- df %>% dplyr::select(-installment)
```

**We chose to remove `installment` from the data since it is nearly perfectly collinear with `loan_amnt`, `loan_amnt` seems more fundamental than `installment`, and `installment` could most likely be calculated using `lona_amnt`, `term`, and `int_rate`.**

```{r}
# remove `title`
df <- df %>% dplyr::select(-title)
```

**We chose to remove `title` from the data since it seems to be a user-entered, more granular (31659 levels > 14 levels), and often repetitive version of `purpose`.**

## Data Wrangling

This section includes recategorizing date variables into meaningful time variables, collapsing factor variables, binning numeric variables, numeric to factor conversions, and feature extraction.

```{r}
# recategorizing `issue_d` into years since
df$issue_d_date <- my(df$issue_d)
df$issue_d_years_since <- year(Sys.Date()) - year(df$issue_d_date) # difference between current year and year of issue date

# recategorizing `issue_d` into months of the year
# df$issue_d_months <- substr(df$issue_d, 0, 3) # pull out 3-letter abbreviation of month (e.g. "Apr")

# table(df$issue_d_months)

# remove `issue_d` and `issue_d_date`
df <- df %>% dplyr::select(-issue_d, -issue_d_date)
```

**`issue_d` has 115 different combinations of months and years spanning from June 2007 to December 2016. To reduce this variable to something more interpretable, we chose to convert the issue date to a "years since" idea, reducing the total levels and assuming the month of loan issue has little to no significance relative to year. Depending on the performance of `issue_d_years_since` in the modeling phase, we may choose to backtrack and convert the issue date to just the month of the issue date instead, reducing the total levels to 12 and assuming the year of the loan issue has little to no significance relative to month. This stems from the idea that lenders may need to meet a certain loan amount/quantity quota for quarters or seasons within a year, sparking a change of behavior when approving loans.**

```{r}
# log transforming `open_acc`
# df$open_acc_log <- log1p(df$open_acc)

# capping `open_acc`
# quantile(df$open_acc, probs = seq(0.9, 1, 0.001)) # print quantiles to visually identify cutoffs
# cap_value <- quantile(df$open_acc, 0.998) # assign value at given quantile
# df$open_acc_cap <- ifelse(df$open_acc > cap_value, cap_value, df$open_acc)

# combining `open_acc` into 10 semi-equal width bins
df$open_acc_bins <- cut(
  df$open_acc,
  breaks = quantile(df$open_acc, probs = seq(0, 1, 0.1)),
  include.lowest = TRUE
)

table(df$open_acc_bins)
```

**`open_acc` is very right-skewed with a couple large outliers. We chose to bin `open_acc` into 10 relatively equal-width bins to reduce the total levels and retain interpretability. Depending on the performance of `open_acc_bins` in the modeling phase, we may choose to backtrack and either perform a log1p (accounting for values of "0") transformation on `open_acc` to make the distribution close to normal and more symmetric, or cap `open_acc` to the 99.8th percentile to limit the influence of the large outliers on future analysis.**

```{r}
# recategorizing `earliest_cr_line` into years since
df$earliest_cr_line_date <- my(df$earliest_cr_line)
df$earliest_cr_line_years_since <- year(Sys.Date()) - year(df$earliest_cr_line_date) # difference between current year and year of earliest credit line

# combining `earliest_cr_line_years_since` into 10 semi-equal width bins
df$earliest_cr_line_bins <- cut(
  df$earliest_cr_line_years_since,
  breaks = quantile(df$earliest_cr_line_years_since, probs = seq(0, 1, 0.1)),
  include.lowest = TRUE
)

table(df$earliest_cr_line_bins)

# remove `earliest_cr_line` and `earliest_cr_line_date`
df <- df %>% dplyr::select(-earliest_cr_line, -earliest_cr_line_date, -earliest_cr_line_years_since)
```

**`earliest_cr_line` has 659 different combinations of months and years spanning from November 1950 to October 2013. To reduce this variable to something more interpretable, we chose to convert the issue date to a "years since" idea, reducing the total levels and assuming the month of earliest credit line has little to no significance relative to year. Additionally, we chose to bin these "years since" counts into 10 relatively equal-width bins to again reduce the total levels and retain interpretability.**

```{r}
# convert `address` from factor to character
df$address <- as.character(df$address)

# pull out `zip_code` from `address`
# df$zip_code <- factor(substr(df$address, nchar(df$address) - 4, nchar(df$address))) # zip code is located in the last 5 indexes

# table(df$zip_code)

# pull out `territory` from `address`
df$territory <- factor(substr(df$address, nchar(df$address) - 7, nchar(df$address) - 6)) # territory is located within the indexes (-7,-6)

# remove `address`
df <- df %>% dplyr::select(-address)
```

**`address` is made up of 99.58% unique values. In order to treat `address` as a factor with a reasonable number of levels, we chose to pull out the territories from each address and use `territory` as a substitute for location. In addition to the 50 U.S. states, "DC" (District of Columbia), "AA" (Armed Forces America), "AE" (Armed Forces Europe, Middle East, Africa, Canada), and "AP" (Armed Forces Pacific) are included as territories in the data. We also considered pulling out zip code as a more granular location variable, but there seems to be a data entry error as only 10 unique zip codes are found in the data despite there being 54 territories.**

```{r}
# recode `delinq_2yrs` into 4 levels: "0", "1", "2", "3+"
df$delinq_2yrs <- forcats::fct_collapse(
  df$delinq_2yrs %>% as.character(), 
  "0" = "0",
  "1" = "1",
  "2" = "2",
  "3+" = as.character(3:max(df$delinq_2yrs))) # range from 3 to the max value of the variable

# reorder levels of `delinq_2yrs`
df$delinq_2yrs <- forcats::fct_relevel(df$delinq_2yrs, "0", "1", "2", "3+")

table(df$delinq_2yrs)
```

**`delinq_2yrs` is zero-dominated and thus right-skewed. To combat this, we chose to convert `delinq_2yrs` to factor and collapse the categories into 4 levels: "0", "1", "2", and "3+". We did this to reduce the number of levels, mitigate data sparsity, and classify large values of `delinq_2yrs` as equal.**

```{r}
# recode `inq_last_6mths` into 3 levels: "0", "1", "2+"
df$inq_last_6mths <- forcats::fct_collapse(
  df$inq_last_6mths %>% as.character(), 
  "0" = "0",
  "1" = "1",
  "2+" = as.character(2:max(df$inq_last_6mths))) # range from 2 to the max value of the variable

# reorder the levels of `inq_last_6mths`
df$inq_last_6mths <- forcats::fct_relevel(df$inq_last_6mths, "0", "1", "2+")

table(df$inq_last_6mths)
```

**`inq_last_6mths` is zero-dominated and right-skewed. To combat this, we chose to convert `inq_last_6mths` to factor and collapse the categories into 3 levels: "0", "1", and "12". We did this to reduce the number of levels, mitigate data sparsity, and classify larger values of `inq_last_6mths` as equal.**

```{r}
# recode `acc_now_delinq` into 2 levels: "0", "1+"
df$acc_now_delinq <- forcats::fct_collapse(
  df$acc_now_delinq %>% as.character(), 
  "0" = "0",
  "1+" = as.character(1:max(as.numeric(df$acc_now_delinq))))  # range from 1 to the max value of the variable

# reorder the levels of `acc_now_delinq`
df$acc_now_delinq <- forcats::fct_relevel(df$acc_now_delinq, "0", "1+")

table(df$acc_now_delinq)
```

**`acc_now_delinq` is extremely zero-dominated and right-skewed. To combat this, we chose to convert `acc_now_delinq` to factor and collapse the categories into 2 levels: "0", ">0". We did this to reduce the number of levels, mitigate data sparsity, and classify larger values of `acc_now_delinq` as equal.**

## Feature Engineering

This section includes slicing variables, performing log transformations, capping numeric variables, creating difference variables, creating ratio features, binning numeric variables, removing observations, imputing missing values, creating average variables, collapsing factor variables, and category mapping.

```{r}
# remove `grade`
df <- df %>% dplyr::select(-grade)

# remove lettering from `sub_grade`
# df$sub_grade_num <- as.numeric(gsub("[A-G]", "", df$sub_grade)) # substitute all letters A through G with empty space and treat resulting value as continuous

# remove `sub_grade`
# df <- df %>% dplyr::select(-sub_grade)
```

**`grade` has 7 levels with values of "A" through "G". `sub_grade` adds a number from 1 to 5 on top of these alphabetical values as a more granular risk rating within each grade. We chose to drop `grade` as the values of `grade` are included within `sub_grade`, therefore all of the predictive power within `grade` is theoretically captured by the more granular `sub_grade`. Depending on the performance of `grade` in the modeling phase, we may choose to backtrack and remove the alphabetical values from `sub_grade` as they are already included in `grade`, allowing `sub_grade_num` to be treated as continuous.**

```{r}
# log transforming `annual_inc`
df$annual_inc_log <- log1p(df$annual_inc)

# capping `annual_inc`
# quantile(df$annual_inc, probs = seq(0.9, 1, 0.001)) # print quantiles to visually identify cutoffs
# cap_value <- quantile(df$annual_inc, 0.995) # assign value of given quantile
# df$annual_inc_cap <- ifelse(df$annual_inc > cap_value, cap_value, df$annual_inc)

# remove `annual_inc`
df <- df %>% dplyr::select(-annual_inc)
```

**`annual_inc` is severely right-skewed with many large outliers. We chose to perform a log transformation on `annual_inc` to make the distribution close to normal and more symmetric. Depending on the performance of `annual_inc_log` in the modeling phase, we may choose to backtrack and cap `annual_inc` to the 99.5th percentile instead to limit the influence of the extreme outliers on future analysis.**

```{r}
# remove observation with value of "9999.00" for `dti`
df <- df[-96957, ]

# log transforming `dti`
# df$dti_log <- log1p(df$dti)

# capping `dti`
# quantile(df$dti, probs = seq(0.9, 1, 0.001)) # print quantiles to visually identify cutoffs
cap_value <- quantile(df$dti, 0.999) # assign value of given quantile
df$dti_cap <- ifelse(df$dti > cap_value, cap_value, df$dti)

# remove `dti`
df <- df %>% dplyr::select(-dti)
```

**`dti` is severely right-skewed with a couple large outliers. We chose to remove observation number 96957 with a value of "9999.00" because this value corresponds to infinity due to a reported value of 0 for `annaul_inc`. Additionally, we chose to cap `dti` to the 99.9th percentile to limit the influence of the extreme outliers on future analysis. Depending on the performance of `dti_cap` in the modeling phase, we may choose to backtrack and perform a log1p (accounting for values of "0") transformation on `dti` to make the distribution close to normal and more symmetric.**

```{r}
# log transforming `revol_bal`
df$revol_bal_log <- log1p(df$revol_bal)

# capping `revol_bal`
# quantile(df$revol_bal, probs = seq(0.9, 1, 0.001)) # print quantiles to visually identify cutoffs
# cap_value <- quantile(df$revol_bal, 0.996) # assign value of given quantile
# df$revol_bal_cap <- ifelse(df$revol_bal > cap_value, cap_value, df$revol_bal)

# remove `revol_bal`
df <- df %>% dplyr::select(-revol_bal)
```

**`revol_bal` is severely right-skewed with many large outliers. We chose to perform a log transformation on `revol_bal` to make the distribution close to normal and more symmetric. Depending on the performance of `revol_bal_log` in the modeling phase, we may choose to backtrack and cap `revol_bal` to the 99.6th percentile instead to limit the influence of the extreme outliers on future analysis.**

```{r}
# impute missing values of `pub_rec_bankruptcies` as 0
df$pub_rec_bankruptcies <- ifelse(is.na(df$pub_rec_bankruptcies), 0, df$pub_rec_bankruptcies)

# create `pub_rec_non_bankruptcies` as difference between `pub_rec` and `pub_rec_bankruptcies`
df$pub_rec_non_bankruptcies <- df$pub_rec - df$pub_rec_bankruptcies

# recode `pub_rec_non_bankruptcies` into 2 levels: "0", "1+"
df$pub_rec_non_bankruptcies <- forcats::fct_collapse(
  df$pub_rec_non_bankruptcies %>% as.character(), 
  "0" = "0",
  "1+"= as.character(1:max(df$pub_rec_non_bankruptcies))) # range from 1 to the max value of the variable

table(df$pub_rec_non_bankruptcies)

# recode `pub_rec_bankruptcies` into 3 levels: "0", "1", "2+"
df$pub_rec_bankruptcies <- forcats::fct_collapse(
  df$pub_rec_bankruptcies %>% as.character(), 
  "0" = "0",
  "1" = "1",
  "2+" = as.character(2:max(df$pub_rec_bankruptcies))) # range from 2 to the max value of the variable

# remove `pub_rec`
df <- df %>% dplyr::select(-pub_rec)

table(df$pub_rec_bankruptcies)
```

**There are 336 missing values in `pub_rec_bankruptcies`. We chose to impute these missing values with zeroes as the majority of the missing values in `pub_rec_bankruptcies` corresponded to a value of 0 for `pub_rec`, and `pub_rec` by nature includes `pub_rec_bankruptcies`. We then chose to create `pub_rec_non_bankruptcies` as the difference between `pub_rec` and `pub_rec_bankruptcies` to replace `pub_rec` and accommodate for double-counting bankruptcies and any multicollinearity between `pub_rec` and `pub_rec_bankruptcies`. `pub_rec_non_bankruptcies` is zero-dominated and thus very right-skewed. To combat this, we chose to convert `pub_rec_non_bankruptcies` to factor and collapse the categories into 2 levels: "0" and "1+". Additionally, `pub_rec_bankruptcies` is zero-dominated and thus very right-skewed. To combat this, we chose to convert `pub_rec_bankruptcies` to factor and collapse the categories into 3 levels: "0", "1", and "2+". We did this to reduce the number of levels and mitigate data sparsity.**

```{r}
# create `closed_acc` as difference between `total_acc` and `open_acc`
df$closed_acc <- df$total_acc - df$open_acc

# log transforming `closed_acc`
# df$closed_acc_log <- log1p(df$closed_acc)

# capping `closed_acc`
# quantile(df$closed_acc, probs = seq(0.9, 1, 0.001)) # print quantiles to visually identify cutoffs
# cap_value <- quantile(df$closed_acc, 0.998) # assign value of given quantile
# df$closed_acc_cap <- ifelse(df$closed_acc > cap_value, cap_value, df$closed_acc)

# combining `closed_acc` into 10 semi-equal width bins
df$closed_acc_bins <- cut(
  df$closed_acc,
  breaks = quantile(df$closed_acc, probs = seq(0, 1, 0.1)),
  include.lowest = TRUE
)

# remove `open_acc`, `total_acc`, and `closed_acc`
df <- df %>% dplyr::select(-open_acc, total_acc, -closed_acc)

table(df$closed_acc_bins)
```

**We then chose to create `closed_acc` as the difference between `pub_rec` and `total_acc` to replace `open_acc` and accommodate for double-counting open accounts and any multicollinearity between `total_acc` and `open_acc`. `total_acc` is zero-dominated and thus very right-skewed. `closed_acc` is very right-skewed with a couple large outliers. We chose to bin `closed_acc` into 10 relatively equal-width bins to reduce the total levels and retain interpretability. Depending on the performance of `open_acc_bins` in the modeling phase, we may choose to backtrack and either perform a log1p (accounting for values of "0") transformation on `open_acc` to make the distribution close to normal and more symmetric, or cap `open_acc` to the 99.8th percentile to limit the influence of the large outliers on future analysis.**

```{r}
# impute missing values of `revol_util` as 0
df$revol_util <- ifelse(is.na(df$revol_util), 0, df$revol_util)

# capping `revol_util`
# quantile(df$revol_util, probs = seq(0.9, 1, 0.001)) # print quantiles to visually identify cutoffs
cap_value <- quantile(df$revol_util, 0.999) # assign value of given quantile
df$revol_util_cap <- ifelse(df$revol_util > cap_value, cap_value, df$revol_util)

# remove `revol_util`
df <- df %>% dplyr::select(-revol_util)
```

**There are 177 missing values in `revol_util`. We chose to impute these missing values with zeroes as the majority of the missing values in `revol_util` corresponded to a value of 0 for `revol_bal`. Likewise, the majority of the values of 0 in `revol_util` corresponded to a value of 0 for `revol_bal` as well, so it made logical sense to treat the missing values as zeroes. `revol_util` is also very right-skewed with a couple large outliers. We chose to cap `revol_util` to the 99.9th percentile to limit the influence of the large outliers on future analysis.**

```{r}
# calculate average of FICO score range
df$fico_range_avg <- (df$fico_range_high + df$fico_range_low) / 2

# remove `fico_range_low` and `fico_range_high`
df <- df %>% dplyr::select(-fico_range_low, -fico_range_high)
```

**Each observation has a value for `fico_range_low` and `fico_range_high` with a difference of either 4 or 5. To reduce the two perfectly correlated variables into one, we chose to take the average of the range.**

```{r}
# recode `mths_since_last_delinq` as factor
df$mths_since_last_delinq <- factor(df$mths_since_last_delinq)

# impute missing values of `mths_since_last_delinq as "Never"
df$mths_since_last_delinq <- ifelse(is.na(df$mths_since_last_delinq), "Never", df$mths_since_last_delinq)

# recode `mths_since_last_delinq` to year ranges from character-converted months
df$yrs_since_last_delinq <- df$mths_since_last_delinq %>%
  as.character() %>%
  fct_collapse(
    "Never" = "Never",
    "<1yr" = as.character(0:11),
    "1-2yr" = as.character(12:23),
    "2-3yr" = as.character(24:35),
    "3-4yr" = as.character(36:47),
    "4-5yr" = as.character(48:59),
    "5-6yr" = as.character(60:71),
    ">6yr" = as.character(72:max(as.numeric(df$mths_since_last_delinq[df$mths_since_last_delinq != "Never"])))) # range from 72 to the max value of the variable

# reorder the levels of `yrs_since_last_delinq`
df$yrs_since_last_delinq <- forcats::fct_relevel(df$yrs_since_last_delinq, "Never", "<1yr", "1-2yr", "2-3yr", "3-4yr", "4-5yr", "5-6yr", ">6yr")

# remove `mths_since_last_delinq`
df <- df %>% dplyr::select(-mths_since_last_delinq)

table(df$yrs_since_last_delinq)
```

**There are 126,474 missing values in `mths_since_last_delinq`. We chose to impute these missing values with "Never" as the missing values in `mths_since_last_delinq` correspond to a loan applicant never having a delinquency. Additionally, we chose to recode `mths_since_last_delinq` into year ranges to reduce the number of levels and mitigate data sparsity; grouping all values greater than 6 years into one level was done as an attempt to create semi-equally-weighted levels and resolve data sparsity.**

```{r}
# recategorizing `last_credit_pull_d` into years since
df$last_credit_pull_d_date <- my(df$last_credit_pull_d)
df$last_credit_pull_d_years_since <- year(Sys.Date()) - year(df$last_credit_pull_d_date) # difference between current year and year of issue date

# impute blank values of `last_credit_pull_d_years_since as "Never"
df$last_credit_pull_d_years_since <- factor(ifelse(is.na(df$last_credit_pull_d_years_since), "Never", df$last_credit_pull_d_years_since))

# regroup `last_credit_pull_d_years_since` to year ranges
df$last_credit_pull_d_years_since <- df$last_credit_pull_d_years_since %>%
  as.character() %>%
  fct_collapse(
    "Never" = "Never",
    "6" = "6",
    "7" = "7",
    "8" = "8",
    "9" = "9",
    "10+" = as.character(10:18)) # range from 10 to 18

# reorder the levels of `last_credit_pull_d_years_since`
df$last_credit_pull_d_years_since <- forcats::fct_relevel(df$last_credit_pull_d_years_since, "Never", "6", "7", "8", "9", "10+")

# remove `last_credit_pull_d` and `last_credit_pull_d_date`
df <- df %>% dplyr::select(-last_credit_pull_d, -last_credit_pull_d_date)

table(df$last_credit_pull_d_years_since)
```

**In addition to the date values of `last_credit_pull_d`, there are 12 blank values. We chose to replace these blank values with "Never" as an indicator of a loan applicant never having their credit pulled. `last_credit_pull_d` has 137 different combinations of months and years spanning from June 2007 to March 2019. To reduce this variable to something more interpretable, we chose to convert the issue date to a "years since" idea, reducing the total levels and assuming the month of last credit pull has little to no significance relative to year. Additionally, we chose to recode `last_credit_pull_d_years_since` into year ranges to again reduce the number of levels and mitigate data sparsity.**

```{r}
# clean `emp_title`
df$emp_title_clean <- df$emp_title %>%
  as.character() %>% # convert to character
  str_trim() %>% # remove all trailing or leading spaces
  str_to_lower() # convert all letters to lowercase

# count the alphabetical, non-modifier words in `emp_title_clean`
word_counts <- df %>%
  select(emp_title_clean) %>%
  unnest_tokens(word, emp_title_clean) %>% # split into words
  filter(!word %in% stop_words$word, # remove common prepositions, conjunctions, and modifiers
         str_detect(word, "[a-z]")) %>% # keep only letters
  count(word, sort = TRUE)

# grab the 200 most common words
common_words <- head(word_counts, 200)

# group `emp_title` into broad categories based on the most common words by pattern matching using regex()
df$emp_title_group <- case_when(
  str_detect(df$emp_title_clean, regex("teach|school|professor|instructor|principal|university|education|college", ignore_case = TRUE)) ~ "Education",
  str_detect(df$emp_title_clean, regex("nurse|rn|clinical|therapist|physician|healthcare|dental|pharmacy|technologist|nursing", ignore_case = TRUE)) ~ "Healthcare",
  str_detect(df$emp_title_clean, regex("driver|truck|mechanic|transportation|bus|logistics|carrier", ignore_case = TRUE)) ~ "Transportation",
  str_detect(df$emp_title_clean, regex("manager|president|vp|vice|chief|director|supervisor|leader|executive|superintendent", ignore_case = TRUE)) ~ "Management",
  str_detect(df$emp_title_clean, regex("account|accountant|accounting|accounts|banker|finance|financial|controller|auditor|credit|loan|insurance|mortgage|underwriter|claims|tax", ignore_case = TRUE)) ~ "Business",
  str_detect(df$emp_title_clean, regex("sales|sale|rep|representative|customer|client|advisor|agent|marketing|store|associate", ignore_case = TRUE)) ~ "Sales",
  str_detect(df$emp_title_clean, regex("clerk|coordinator|secretary|administrative|admin|office|support|assistant|asst", ignore_case = TRUE)) ~ "Administration",
  str_detect(df$emp_title_clean, regex("engineer|engineering|software|programmer|developer|technician|technical|tech|technology|network|data|it|system|computer|design|designer", ignore_case = TRUE)) ~ "Tech",
  str_detect(df$emp_title_clean, regex("police|sheriff|fire|sergeant|correctional|army|military|navy|federal|public|postal|government", ignore_case = TRUE)) ~ "Public/Government",
  str_detect(df$emp_title_clean, regex("construction|electrician|foreman|mechanic|maintenance|equipment|operator|machin|labor|inspector|mech", ignore_case = TRUE)) ~ "Trades",
  str_detect(df$emp_title_clean, regex("attorney|paralegal|legal|compliance", ignore_case = TRUE)) ~ "Legal",
  str_detect(df$emp_title_clean, regex("owner|self|llc|corp|corporation|company|associates|llp", ignore_case = TRUE)) ~ "Self-Employed",
  TRUE ~ "Other"
)

# remove `emp_title` and `emp_title_clean`
df <- df %>% dplyr::select(-emp_title, -emp_title_clean)

table(df$emp_title_group)
```

**`emp_title` is another user-entered variable with 112,646 different unique values. We chose to combine values containing the most common words/strings into a much smaller set of broad categories to hopefully gain predictive power. Disclaimer: ChatGPT was leveraged to help with the syntax for counting words and for the tediousness of mapping words to categories.**

```{r}
# set the level with the highest frequency as the base level for `emp_title_group`
df$emp_title_group <- forcats::fct_relevel(df$emp_title_group, names(which.max(table(df$emp_title_group))))

levels(df$emp_title_group)
```

**We chose to relevel `emp_title_group` so that the highest frequency level ("Other") is set as the base level.**

```{r}
# recode `term` to years for consistency, and remove leading spaces
df$term <- as.factor(ifelse(df$term == " 36 months", "3 Years", "5 Years"))

table(df$term)
```

**We chose to rename the values of `term` from months to years to be consistent with our general feature engineering approach for time variables. Additionally, there was a leading space in front of the original values, so we removed it.**

```{r}
# create missing value indicator of `mort_acc`
df$mort_acc_NA <- ifelse(is.na(df$mort_acc), 1, 0)

# impute missing values of `mort_acc` as median of `home_ownership` groupings
df <- df %>%
  group_by(home_ownership) %>%
  mutate(mort_acc = ifelse(is.na(mort_acc), median(mort_acc, na.rm = TRUE), mort_acc)) %>% # impute median of `mort_acc` for each group
  ungroup()

# recode `mort_acc` into 5 levels
df$mort_acc <- forcats::fct_collapse(
  df$mort_acc %>% as.character(), 
  "0" = "0",
  "1" = "1",
  "2" = "2",
  "3-4" = as.character(3:4),
  "5+" = as.character(5:max(df$mort_acc))) # range from 72 to the max value of the variable

# reorder levels of `mort_acc`
df$mort_acc <- forcats::fct_relevel(df$mort_acc, "0", "1", "2", "3-4", "5+")

table(df$mort_acc)
```

**There are 22854 missing values in `mort_acc`. We chose to impute these missing values with the median of the given level of `home_ownership` for each corresponding observation. Since `mort_acc` seems to logically be related to `home_ownership` in some fashion, we chose to impute using `home_ownership` rather than just simple imputation. `mort_acc` is zero-dominated and thus very right-skewed. To combat this, we chose to convert `mort_acc` to factor and collapse the categories into 5 levels: "0", "1", "2", "3-4", and "5+". We did this to avoid issues with skew, mitigate data sparsity, and classify larger values of `mort_acc` as equal.**

## Additional Comments

write about probable interactions

write about dropping uninformative variables 

```{r}
# display a list of variables in the data
# colnames(df)

# create a set of probable two-variable interactions in the data
df <- df %>%
  mutate(int_mort_home = paste0(home_ownership, "_", mort_acc))

df <- df %>%
  mutate(int_dti_income = dti_cap * annual_inc_log)

df <- df %>%
  mutate(int_fico_loan = fico_range_avg * loan_amnt)

df <- df %>%
  mutate(int_loan_income = loan_amnt * annual_inc_log)
```
